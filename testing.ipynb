{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd73ca8-afd1-41ca-943e-a3598a4a9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c26cb6-f12b-43ea-9482-455cbb4e42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d360dec-8745-4fee-adcf-4aeb6c35be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "\n",
    "qm9 = QM9(root='./', transform=None, pre_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dcd5b5d-54ac-4ffe-903e-c788b55cbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch_geometric.nn as tgnn\n",
    "from graphormer.model import Graphormer\n",
    "\n",
    "\n",
    "model = Graphormer(\n",
    "    num_layers=3,\n",
    "    input_dim=5,\n",
    "    emb_dim=128,\n",
    "    input_edge_attr_dim=4,\n",
    "    edge_attr_dim=16,\n",
    "    output_dim=1,\n",
    "    num_radial=10,\n",
    "    radial_min=0,\n",
    "    radial_max=10,\n",
    "    num_heads=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0b624d-7d7d-423c-acf2-5c1e0d033917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_ids, train_ids = train_test_split([i for i in range(len(qm9))], test_size=0.8, random_state=42)\n",
    "train_loader = DataLoader(Subset(qm9, train_ids), batch_size=64)\n",
    "test_loader = DataLoader(Subset(qm9, test_ids), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af514e6d-cfd9-46b0-8725-45b422f9db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "loss_functin = nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f42e66-2ca7-457d-9218-080bd1400ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 125989504.0\n",
      "LOSS 115867776.0\n",
      "LOSS 106052224.0\n",
      "LOSS 99492432.0\n",
      "LOSS 81012992.0\n",
      "LOSS 74148688.0\n",
      "LOSS 67983800.0\n",
      "LOSS 53378548.0\n",
      "LOSS 41076088.0\n",
      "LOSS 35717424.0\n",
      "LOSS 19474852.0\n",
      "LOSS 8957322.0\n",
      "LOSS 5815762.0\n",
      "LOSS 2303852.5\n",
      "LOSS 1375826.0\n",
      "LOSS 1486422.375\n",
      "LOSS 1143528.25\n",
      "LOSS 852999.375\n",
      "LOSS 1231683.5\n",
      "LOSS 667563.875\n",
      "LOSS 1279514.75\n",
      "LOSS 1078682.625\n",
      "LOSS 1166886.5\n",
      "LOSS 1148435.25\n",
      "LOSS 1313434.25\n",
      "LOSS 1337940.25\n",
      "LOSS 1442692.5\n",
      "LOSS 1378165.5\n",
      "LOSS 997401.875\n",
      "LOSS 1123959.25\n",
      "LOSS 914281.3125\n",
      "LOSS 957916.375\n",
      "LOSS 1664437.5\n",
      "LOSS 1012348.75\n",
      "LOSS 1165281.625\n",
      "LOSS 1237530.0\n",
      "LOSS 1058093.0\n",
      "LOSS 1903017.25\n",
      "LOSS 1210704.75\n",
      "LOSS 1595341.875\n",
      "LOSS 1754575.75\n",
      "LOSS 1284520.625\n",
      "LOSS 1022583.8125\n",
      "LOSS 1411748.5\n",
      "LOSS 1537039.0\n",
      "LOSS 961344.375\n",
      "LOSS 1521828.25\n",
      "LOSS 1446379.25\n",
      "LOSS 950908.375\n",
      "LOSS 1124355.875\n",
      "LOSS 786653.8125\n",
      "LOSS 1413205.5\n",
      "LOSS 1297839.25\n",
      "LOSS 1167053.5\n",
      "LOSS 1434335.375\n",
      "LOSS 1193116.0\n",
      "LOSS 1504906.75\n",
      "LOSS 1057144.25\n",
      "LOSS 938360.375\n",
      "LOSS 1147558.0\n",
      "LOSS 1212051.25\n",
      "LOSS 1833844.0\n",
      "LOSS 1921363.25\n",
      "LOSS 1090013.75\n",
      "LOSS 1318556.5\n",
      "LOSS 1675894.25\n",
      "LOSS 1484002.0\n",
      "LOSS 1040933.0625\n",
      "LOSS 991380.375\n",
      "LOSS 1013831.875\n",
      "LOSS 1023184.0625\n",
      "LOSS 1179173.875\n",
      "LOSS 1091883.5\n",
      "LOSS 1471914.75\n",
      "LOSS 1409252.125\n",
      "LOSS 750390.375\n",
      "LOSS 1251788.5\n",
      "LOSS 997013.625\n",
      "LOSS 884032.0\n",
      "LOSS 808129.75\n",
      "LOSS 1334841.25\n",
      "LOSS 1056624.25\n",
      "LOSS 1042892.6875\n",
      "LOSS 1020883.5625\n",
      "LOSS 1506020.125\n",
      "LOSS 869602.5\n",
      "LOSS 908101.3125\n",
      "LOSS 1803879.25\n",
      "LOSS 1180492.25\n",
      "LOSS 885487.375\n",
      "LOSS 966219.375\n",
      "LOSS 969162.0\n",
      "LOSS 1263953.0\n",
      "LOSS 1377863.875\n",
      "LOSS 960877.125\n",
      "LOSS 1246796.25\n",
      "LOSS 1633864.75\n",
      "LOSS 1139830.875\n",
      "LOSS 1701695.5\n",
      "LOSS 1188796.0\n",
      "LOSS 1294890.625\n",
      "LOSS 1870925.875\n",
      "LOSS 1119868.0\n",
      "LOSS 1860513.5\n",
      "LOSS 1083599.25\n",
      "LOSS 1145468.625\n",
      "LOSS 1099022.5\n",
      "LOSS 1297204.75\n",
      "LOSS 1509295.25\n",
      "LOSS 1700398.5\n",
      "LOSS 1251086.625\n",
      "LOSS 1064311.125\n",
      "LOSS 1241147.375\n",
      "LOSS 1298873.0\n",
      "LOSS 884245.375\n",
      "LOSS 844082.125\n",
      "LOSS 1091436.375\n",
      "LOSS 1056503.75\n",
      "LOSS 1333534.75\n",
      "LOSS 1334221.75\n",
      "LOSS 1879250.25\n",
      "LOSS 1744379.5\n",
      "LOSS 1316519.5\n",
      "LOSS 952122.5625\n",
      "LOSS 1856431.25\n",
      "LOSS 1379064.125\n",
      "LOSS 1496337.0\n",
      "LOSS 1298160.0\n",
      "LOSS 1777525.75\n",
      "LOSS 1442763.75\n",
      "LOSS 918350.25\n",
      "LOSS 932602.25\n",
      "LOSS 1869921.5\n",
      "LOSS 2054401.5\n",
      "LOSS 1060390.125\n",
      "LOSS 1090813.75\n",
      "LOSS 866575.5\n",
      "LOSS 1420716.75\n",
      "LOSS 1192534.875\n",
      "LOSS 1596085.25\n",
      "LOSS 859065.125\n",
      "LOSS 1289438.5\n",
      "LOSS 1032515.25\n",
      "LOSS 1441331.625\n",
      "LOSS 738861.875\n",
      "LOSS 1392075.125\n",
      "LOSS 899154.125\n",
      "LOSS 1215722.125\n",
      "LOSS 1111927.0\n",
      "LOSS 901024.375\n",
      "LOSS 1159088.25\n",
      "LOSS 1048213.0\n",
      "LOSS 1130560.0\n",
      "LOSS 1178563.625\n",
      "LOSS 1118662.75\n",
      "LOSS 1384839.125\n",
      "LOSS 1699529.5\n",
      "LOSS 1584906.625\n",
      "LOSS 1227350.75\n",
      "LOSS 1320505.75\n",
      "LOSS 942695.125\n",
      "LOSS 1643701.25\n",
      "LOSS 1631322.25\n",
      "LOSS 1226269.5\n",
      "TRAIN_LOSS 5919362.630258444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:18<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL LOSS 1286762.841397233\n",
      "LOSS 1053508.5\n",
      "LOSS 771471.8125\n",
      "LOSS 809538.625\n",
      "LOSS 910274.375\n",
      "LOSS 1975319.125\n",
      "LOSS 1540618.25\n",
      "LOSS 1178334.75\n",
      "LOSS 1200442.0\n",
      "LOSS 2360203.25\n",
      "LOSS 1386791.375\n",
      "LOSS 1031284.4375\n",
      "LOSS 1116211.0\n",
      "LOSS 1664225.5\n",
      "LOSS 1313573.625\n",
      "LOSS 1156836.25\n",
      "LOSS 1554585.5\n",
      "LOSS 1355772.0\n",
      "LOSS 859233.625\n",
      "LOSS 1238049.5\n",
      "LOSS 682777.25\n",
      "LOSS 1338279.25\n",
      "LOSS 1144232.75\n",
      "LOSS 1147202.25\n",
      "LOSS 1151116.75\n",
      "LOSS 1302691.625\n",
      "LOSS 1324457.5\n",
      "LOSS 1515842.25\n",
      "LOSS 1407987.75\n",
      "LOSS 1065332.0\n",
      "LOSS 1121113.875\n",
      "LOSS 974077.625\n",
      "LOSS 882390.625\n",
      "LOSS 1659560.125\n",
      "LOSS 1094194.0\n",
      "LOSS 1183420.0\n",
      "LOSS 1258961.75\n",
      "LOSS 1046128.5625\n",
      "LOSS 1888862.25\n",
      "LOSS 1145642.0\n",
      "LOSS 1578507.75\n",
      "LOSS 1847499.0\n",
      "LOSS 1375171.75\n",
      "LOSS 997508.9375\n",
      "LOSS 1416063.25\n",
      "LOSS 1541481.25\n",
      "LOSS 1024271.9375\n",
      "LOSS 1523158.125\n",
      "LOSS 1345392.0\n",
      "LOSS 948938.625\n",
      "LOSS 1081888.25\n",
      "LOSS 753246.1875\n",
      "LOSS 1388248.75\n",
      "LOSS 1308362.0\n",
      "LOSS 1208069.75\n",
      "LOSS 1436563.25\n",
      "LOSS 1206327.75\n",
      "LOSS 1356524.375\n",
      "LOSS 1023808.0\n",
      "LOSS 986289.625\n",
      "LOSS 1111279.25\n",
      "LOSS 1183772.25\n",
      "LOSS 1812402.75\n",
      "LOSS 1953611.25\n",
      "LOSS 1100556.875\n",
      "LOSS 1277821.375\n",
      "LOSS 1689778.625\n",
      "LOSS 1450313.25\n",
      "LOSS 1049485.0\n",
      "LOSS 989535.8125\n",
      "LOSS 1089032.5\n",
      "LOSS 981181.6875\n",
      "LOSS 1165567.75\n",
      "LOSS 1089735.5\n",
      "LOSS 1375850.0\n",
      "LOSS 1417140.625\n",
      "LOSS 763488.0\n",
      "LOSS 1215833.0\n",
      "LOSS 997560.0625\n",
      "LOSS 976996.9375\n",
      "LOSS 799603.5\n",
      "LOSS 1345487.625\n",
      "LOSS 1095621.5\n",
      "LOSS 1068686.75\n",
      "LOSS 1020389.875\n",
      "LOSS 1404346.25\n",
      "LOSS 905737.75\n",
      "LOSS 966813.4375\n",
      "LOSS 1864404.75\n",
      "LOSS 1165247.375\n",
      "LOSS 889459.1875\n",
      "LOSS 947931.375\n",
      "LOSS 968964.375\n",
      "LOSS 1262097.75\n",
      "LOSS 1367773.25\n",
      "LOSS 956693.9375\n",
      "LOSS 1193185.0\n",
      "LOSS 1611554.75\n",
      "LOSS 1286751.5\n",
      "LOSS 1591778.625\n",
      "LOSS 1290160.5\n",
      "LOSS 1357438.375\n",
      "LOSS 1881374.5\n",
      "LOSS 1121699.375\n",
      "LOSS 2123533.75\n",
      "LOSS 1149645.375\n",
      "LOSS 1225221.5\n",
      "LOSS 1225977.25\n",
      "LOSS 1297239.25\n",
      "LOSS 1497637.0\n",
      "LOSS 1701055.125\n",
      "LOSS 1240965.125\n",
      "LOSS 1058767.375\n",
      "LOSS 1228058.5\n",
      "LOSS 1305006.5\n",
      "LOSS 870435.1875\n",
      "LOSS 832180.625\n",
      "LOSS 1088385.125\n",
      "LOSS 1100915.0\n",
      "LOSS 1336396.625\n",
      "LOSS 1321060.875\n",
      "LOSS 1856647.875\n",
      "LOSS 1764452.125\n",
      "LOSS 1422006.875\n",
      "LOSS 889519.0\n",
      "LOSS 1910095.0\n",
      "LOSS 1541619.5\n",
      "LOSS 1572806.625\n",
      "LOSS 1327010.75\n",
      "LOSS 1800305.5\n",
      "LOSS 1435845.25\n",
      "LOSS 870083.25\n",
      "LOSS 891417.9375\n",
      "LOSS 1789822.75\n",
      "LOSS 2088150.125\n",
      "LOSS 887866.375\n",
      "LOSS 1087786.5\n",
      "LOSS 940925.75\n",
      "LOSS 1417481.375\n",
      "LOSS 1186344.875\n",
      "LOSS 1595558.25\n",
      "LOSS 856343.0\n",
      "LOSS 1290371.125\n",
      "LOSS 1029900.25\n",
      "LOSS 1480014.0\n",
      "LOSS 780557.75\n",
      "LOSS 1388665.75\n",
      "LOSS 928095.5\n",
      "LOSS 1233092.375\n",
      "LOSS 1115283.75\n",
      "LOSS 888408.875\n",
      "LOSS 1234777.0\n",
      "LOSS 1013538.375\n",
      "LOSS 1142928.0\n",
      "LOSS 1198183.75\n",
      "LOSS 1568634.25\n",
      "LOSS 2379038.75\n",
      "LOSS 1224630.5\n",
      "LOSS 1536665.75\n",
      "LOSS 1370202.0\n",
      "LOSS 1380264.0\n",
      "LOSS 911943.75\n",
      "LOSS 1638859.625\n",
      "LOSS 1644214.875\n",
      "LOSS 1219309.75\n",
      "TRAIN_LOSS 1281006.7940954473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:15<00:00, 26.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL LOSS 1295548.0493770542\n",
      "LOSS 1055480.625\n",
      "LOSS 771214.625\n",
      "LOSS 830544.0\n",
      "LOSS 903456.3125\n",
      "LOSS 1969276.625\n",
      "LOSS 1544384.625\n",
      "LOSS 1170258.0\n",
      "LOSS 1205579.75\n",
      "LOSS 2364878.0\n",
      "LOSS 1374504.0\n",
      "LOSS 1029308.9375\n",
      "LOSS 1091530.375\n",
      "LOSS 1655970.5\n",
      "LOSS 1313538.875\n",
      "LOSS 1157654.875\n",
      "LOSS 1578203.75\n",
      "LOSS 1355252.0\n",
      "LOSS 813174.125\n",
      "LOSS 1232024.75\n",
      "LOSS 697403.5625\n",
      "LOSS 1323875.875\n",
      "LOSS 1187870.625\n",
      "LOSS 1154526.0\n",
      "LOSS 1146803.125\n",
      "LOSS 1313956.5\n",
      "LOSS 1318671.125\n",
      "LOSS 1543991.5\n",
      "LOSS 1398674.75\n",
      "LOSS 1085521.0\n",
      "LOSS 1130304.125\n",
      "LOSS 960623.125\n",
      "LOSS 858993.5625\n",
      "LOSS 1590680.25\n",
      "LOSS 1090940.0\n",
      "LOSS 1193480.5\n",
      "LOSS 1278830.0\n",
      "LOSS 1039464.25\n",
      "LOSS 1857345.625\n",
      "LOSS 1139411.5\n",
      "LOSS 1606492.25\n",
      "LOSS 1869466.5\n",
      "LOSS 1393805.75\n",
      "LOSS 1001390.5625\n",
      "LOSS 1416204.0\n",
      "LOSS 1534687.25\n",
      "LOSS 1046036.9375\n",
      "LOSS 1499592.5\n",
      "LOSS 1337200.25\n",
      "LOSS 946318.625\n",
      "LOSS 1073577.375\n",
      "LOSS 773575.125\n",
      "LOSS 1394116.5\n",
      "LOSS 1297809.25\n",
      "LOSS 1205472.25\n",
      "LOSS 1443639.5\n",
      "LOSS 1203087.75\n",
      "LOSS 1319587.375\n",
      "LOSS 1020574.3125\n",
      "LOSS 996905.8125\n",
      "LOSS 1120815.25\n",
      "LOSS 1169052.5\n",
      "LOSS 1774103.875\n",
      "LOSS 1952771.375\n",
      "LOSS 1103597.5\n",
      "LOSS 2639519.0\n",
      "LOSS 2105962.75\n",
      "LOSS 1345249.0\n",
      "LOSS 1116917.75\n",
      "LOSS 993811.5\n",
      "LOSS 1016448.625\n",
      "LOSS 1000877.125\n",
      "LOSS 1167507.125\n",
      "LOSS 1081114.875\n",
      "LOSS 1413491.75\n",
      "LOSS 1415071.0\n",
      "LOSS 756490.0\n",
      "LOSS 1218476.0\n",
      "LOSS 1002236.0625\n",
      "LOSS 941626.5\n",
      "LOSS 805037.0\n",
      "LOSS 1342788.25\n",
      "LOSS 1106197.75\n",
      "LOSS 1055639.875\n",
      "LOSS 1029978.8125\n",
      "LOSS 1429701.75\n",
      "LOSS 901966.5\n",
      "LOSS 935076.125\n",
      "LOSS 1860527.25\n",
      "LOSS 1165496.625\n",
      "LOSS 880546.0625\n",
      "LOSS 946964.125\n",
      "LOSS 970885.375\n",
      "LOSS 1254397.25\n",
      "LOSS 1361726.375\n",
      "LOSS 952802.0\n",
      "LOSS 1204298.25\n",
      "LOSS 1621036.0\n",
      "LOSS 1228118.125\n",
      "LOSS 1675484.625\n",
      "LOSS 1242282.0\n",
      "LOSS 1304828.0\n",
      "LOSS 1911095.25\n",
      "LOSS 1108304.25\n",
      "LOSS 1973162.0\n",
      "LOSS 1097437.5\n",
      "LOSS 1198985.75\n",
      "LOSS 1145580.5\n",
      "LOSS 1268365.0\n",
      "LOSS 1490387.25\n",
      "LOSS 1696669.75\n",
      "LOSS 1243271.25\n",
      "LOSS 1060225.25\n",
      "LOSS 1223715.75\n",
      "LOSS 1300058.875\n",
      "LOSS 869460.0\n",
      "LOSS 835704.8125\n",
      "LOSS 1089323.875\n",
      "LOSS 1067765.75\n",
      "LOSS 1331748.625\n",
      "LOSS 1332086.0\n",
      "LOSS 1854854.375\n",
      "LOSS 1752949.125\n",
      "LOSS 1363859.0\n",
      "LOSS 925554.875\n",
      "LOSS 1886790.75\n",
      "LOSS 1425665.75\n",
      "LOSS 1491251.125\n",
      "LOSS 1319104.25\n",
      "LOSS 1777661.0\n",
      "LOSS 1450429.125\n",
      "LOSS 885166.9375\n",
      "LOSS 917832.5\n",
      "LOSS 1803462.875\n",
      "LOSS 2091800.875\n",
      "LOSS 951663.5\n",
      "LOSS 1110174.5\n",
      "LOSS 896888.25\n",
      "LOSS 1410004.75\n",
      "LOSS 1188658.75\n",
      "LOSS 1591269.75\n",
      "LOSS 849587.625\n",
      "LOSS 1285346.75\n",
      "LOSS 1020619.8125\n",
      "LOSS 1445671.375\n",
      "LOSS 743653.0\n",
      "LOSS 1381290.75\n",
      "LOSS 900766.1875\n",
      "LOSS 1214656.0\n",
      "LOSS 1110157.0\n",
      "LOSS 890763.375\n",
      "LOSS 1194198.625\n",
      "LOSS 1034511.0625\n",
      "LOSS 1123877.875\n",
      "LOSS 1171501.5\n",
      "LOSS 1126131.125\n",
      "LOSS 1376244.375\n",
      "LOSS 1634587.25\n",
      "LOSS 1601177.25\n",
      "LOSS 1245147.25\n",
      "LOSS 1316324.75\n",
      "LOSS 924713.3125\n",
      "LOSS 1638570.5\n",
      "LOSS 1640340.5\n",
      "LOSS 1219035.75\n",
      "TRAIN_LOSS 1297373.7369703338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:15<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL LOSS 1285173.490483834\n",
      "LOSS 1050643.25\n",
      "LOSS 763190.0\n",
      "LOSS 822113.0\n",
      "LOSS 887309.625\n",
      "LOSS 1975760.75\n",
      "LOSS 1539209.0\n",
      "LOSS 1163099.0\n",
      "LOSS 1203051.5\n",
      "LOSS 2362766.0\n",
      "LOSS 1367309.625\n",
      "LOSS 1026526.875\n",
      "LOSS 1099550.5\n",
      "LOSS 1642013.25\n",
      "LOSS 1310855.5\n",
      "LOSS 1158448.75\n",
      "LOSS 1567104.75\n",
      "LOSS 1347892.5\n",
      "LOSS 823866.375\n",
      "LOSS 1229487.0\n",
      "LOSS 687674.125\n",
      "LOSS 1321990.25\n",
      "LOSS 1163828.75\n",
      "LOSS 1143096.75\n",
      "LOSS 1138430.875\n",
      "LOSS 1299242.625\n",
      "LOSS 1315443.25\n",
      "LOSS 1528043.875\n",
      "LOSS 1400036.625\n",
      "LOSS 1071237.5\n",
      "LOSS 1126735.75\n",
      "LOSS 959884.3125\n",
      "LOSS 863721.1875\n",
      "LOSS 1619812.25\n",
      "LOSS 1084755.125\n",
      "LOSS 1179961.875\n",
      "LOSS 1258436.75\n",
      "LOSS 1035990.375\n",
      "LOSS 1870282.375\n",
      "LOSS 1137953.375\n",
      "LOSS 1589119.25\n",
      "LOSS 1845904.25\n",
      "LOSS 1377546.0\n",
      "LOSS 995358.3125\n",
      "LOSS 1410440.0\n",
      "LOSS 1533160.125\n",
      "LOSS 1029499.5\n",
      "LOSS 1503097.375\n",
      "LOSS 1339576.75\n",
      "LOSS 941117.9375\n",
      "LOSS 1069057.5\n",
      "LOSS 758130.625\n",
      "LOSS 1393681.75\n",
      "LOSS 1302452.5\n",
      "LOSS 1199475.0\n",
      "LOSS 1439875.75\n",
      "LOSS 1202915.0\n",
      "LOSS 1344602.75\n",
      "LOSS 1014532.4375\n",
      "LOSS 975221.3125\n",
      "LOSS 1103996.25\n",
      "LOSS 1167746.875\n",
      "LOSS 1788002.25\n",
      "LOSS 1954766.75\n",
      "LOSS 1095803.75\n",
      "LOSS 1261955.375\n",
      "LOSS 1677215.5\n",
      "LOSS 1440514.0\n",
      "LOSS 1046089.25\n",
      "LOSS 987359.75\n",
      "LOSS 1082857.5\n",
      "LOSS 974532.3125\n",
      "LOSS 1163312.25\n",
      "LOSS 1075042.875\n",
      "LOSS 1375855.75\n",
      "LOSS 1409210.0\n",
      "LOSS 757224.0\n",
      "LOSS 1209766.25\n",
      "LOSS 996999.125\n",
      "LOSS 963750.375\n",
      "LOSS 798384.125\n",
      "LOSS 1343640.375\n",
      "LOSS 1086366.875\n",
      "LOSS 1063104.125\n",
      "LOSS 1027655.125\n",
      "LOSS 1398390.5\n",
      "LOSS 896615.625\n",
      "LOSS 951350.6875\n",
      "LOSS 1864439.375\n",
      "LOSS 1162232.25\n",
      "LOSS 875435.5\n",
      "LOSS 944546.375\n",
      "LOSS 971364.8125\n",
      "LOSS 1256395.75\n",
      "LOSS 1354530.25\n",
      "LOSS 946060.25\n",
      "LOSS 1192338.375\n",
      "LOSS 1592700.75\n",
      "LOSS 1263926.75\n",
      "LOSS 1603915.875\n",
      "LOSS 1257579.125\n",
      "LOSS 1336530.875\n",
      "LOSS 1889507.0\n",
      "LOSS 1104619.875\n",
      "LOSS 2052841.0\n",
      "LOSS 1114086.375\n",
      "LOSS 1208429.5\n",
      "LOSS 1184450.5\n",
      "LOSS 1279046.125\n",
      "LOSS 1478432.875\n",
      "LOSS 1698892.5\n",
      "LOSS 1241272.625\n",
      "LOSS 1057177.25\n",
      "LOSS 1215854.5\n",
      "LOSS 1299827.25\n",
      "LOSS 865195.25\n",
      "LOSS 829481.6875\n",
      "LOSS 1087233.5\n",
      "LOSS 2954360.75\n",
      "LOSS 2551772.5\n",
      "LOSS 1268338.0\n",
      "LOSS 2306262.75\n",
      "LOSS 1595604.25\n",
      "LOSS 1273778.625\n",
      "LOSS 904075.9375\n",
      "LOSS 1843250.0\n",
      "LOSS 1395000.75\n",
      "LOSS 1505609.0\n",
      "LOSS 1316775.0\n",
      "LOSS 1766267.5\n",
      "LOSS 1449941.0\n",
      "LOSS 894744.0625\n",
      "LOSS 926620.5\n",
      "LOSS 1817179.125\n",
      "LOSS 2082715.0\n",
      "LOSS 988010.375\n",
      "LOSS 1107885.75\n",
      "LOSS 874846.875\n",
      "LOSS 1404772.0\n",
      "LOSS 1191360.0\n",
      "LOSS 1579444.5\n",
      "LOSS 842109.1875\n",
      "LOSS 1281273.625\n",
      "LOSS 1016237.625\n",
      "LOSS 1429366.25\n",
      "LOSS 732051.9375\n",
      "LOSS 1375500.25\n",
      "LOSS 888604.0\n",
      "LOSS 1209420.75\n",
      "LOSS 1104025.625\n",
      "LOSS 887113.5625\n",
      "LOSS 1167847.5\n",
      "LOSS 1039175.25\n",
      "LOSS 1104807.25\n",
      "LOSS 1162565.0\n",
      "LOSS 1123794.5\n",
      "LOSS 1381409.5\n",
      "LOSS 1655880.375\n",
      "LOSS 1588724.0\n",
      "LOSS 1224672.625\n",
      "LOSS 1306860.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/tannin/qvo/MolT/testing.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tannin/qvo/MolT/testing.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m edge_attr[batch\u001b[39m.\u001b[39medge_index[\u001b[39m0\u001b[39m], batch\u001b[39m.\u001b[39medge_index[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39medge_attr\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tannin/qvo/MolT/testing.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m batch\u001b[39m.\u001b[39medge_attr \u001b[39m=\u001b[39m edge_attr\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/tannin/qvo/MolT/testing.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m output \u001b[39m=\u001b[39m global_mean_pool(model(batch), batch\u001b[39m.\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tannin/qvo/MolT/testing.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_functin(output\u001b[39m.\u001b[39msqueeze(), y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tannin/qvo/MolT/testing.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m batch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/media/tannin/qvo/miniconda3/envs/MolT/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tannin/qvo/MolT/graphormer/model.py:92\u001b[0m, in \u001b[0;36mGraphormer.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     89\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m centrality_encoding\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 92\u001b[0m     x \u001b[39m=\u001b[39m layer(x, rb_embedding, edge_attr, ptr)\n\u001b[1;32m     94\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_out_lin(x)\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/media/tannin/qvo/miniconda3/envs/MolT/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tannin/qvo/MolT/graphormer/layers.py:200\u001b[0m, in \u001b[0;36mGraphormerEncoderLayer.forward\u001b[0;34m(self, x, rb_embedding, edge_attr, ptr)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    189\u001b[0m             x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    190\u001b[0m             rb_embedding: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    191\u001b[0m             edge_attr: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    192\u001b[0m             ptr) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    193\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    :param x: node feature matrix\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m    :param rb_embedding: radial basis embedding\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m    :return: torch.Tensor, node embeddings after Graphormer layer operations\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     x_prime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x), x, x, rb_embedding, edge_attr, ptr) \u001b[39m+\u001b[39m x\n\u001b[1;32m    201\u001b[0m     x_new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x_prime)) \u001b[39m+\u001b[39m x_prime\n\u001b[1;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m x_new\n",
      "File \u001b[0;32m/media/tannin/qvo/miniconda3/envs/MolT/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/tannin/qvo/MolT/graphormer/layers.py:138\u001b[0m, in \u001b[0;36mGraphormerMHA.forward\u001b[0;34m(self, query, key, value, rb_embedding, edge_attr, ptr)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m:param query: node feature matrix\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m:param key: node feature matrix\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m:return: torch.Tensor, node embeddings after attention operation\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m block_sizes \u001b[39m=\u001b[39m ptr[\u001b[39m1\u001b[39m:] \u001b[39m-\u001b[39m ptr[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 138\u001b[0m blocks \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mones((size, size)) \u001b[39mfor\u001b[39;00m size \u001b[39min\u001b[39;00m block_sizes]\n\u001b[1;32m    139\u001b[0m block_diag_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mblock_diag(\u001b[39m*\u001b[39mblocks)\n\u001b[1;32m    141\u001b[0m batch_mask_zeros \u001b[39m=\u001b[39m block_diag_matrix\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(query\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/media/tannin/qvo/MolT/graphormer/layers.py:138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m:param query: node feature matrix\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m:param key: node feature matrix\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m:return: torch.Tensor, node embeddings after attention operation\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m block_sizes \u001b[39m=\u001b[39m ptr[\u001b[39m1\u001b[39m:] \u001b[39m-\u001b[39m ptr[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 138\u001b[0m blocks \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mones((size, size)) \u001b[39mfor\u001b[39;00m size \u001b[39min\u001b[39;00m block_sizes]\n\u001b[1;32m    139\u001b[0m block_diag_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mblock_diag(\u001b[39m*\u001b[39mblocks)\n\u001b[1;32m    141\u001b[0m batch_mask_zeros \u001b[39m=\u001b[39m block_diag_matrix\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(query\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model.to(DEVICE)\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    batch_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch.to(DEVICE)\n",
    "        y = batch.y[:, 7]\n",
    "        optimizer.zero_grad()\n",
    "        batch.x = batch.x[:, :5]\n",
    "\n",
    "        edge_attr = torch.zeros(batch.x.shape[0], batch.x.shape[0], 4).to(DEVICE)\n",
    "        edge_attr[batch.edge_index[0], batch.edge_index[1]] = batch.edge_attr\n",
    "\n",
    "        batch.edge_attr = edge_attr\n",
    "\n",
    "        output = global_mean_pool(model(batch), batch.batch)\n",
    "\n",
    "        loss = loss_functin(output.squeeze(), y)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"LOSS\", loss.item() / len(batch.y))\n",
    "    print(\"TRAIN_LOSS\", batch_loss / len(train_ids))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "        for batch in tqdm(test_loader):\n",
    "            batch.to(DEVICE)\n",
    "            y = batch.y[:, 7]\n",
    "            batch.x = batch.x[:, :5]\n",
    "            edge_attr = torch.zeros(batch.x.shape[0], batch.x.shape[0], 4).to(DEVICE)\n",
    "            edge_attr[batch.edge_index[0], batch.edge_index[1]] = batch.edge_attr\n",
    "\n",
    "            batch.edge_attr = edge_attr\n",
    "            with torch.no_grad():\n",
    "                output = global_mean_pool(model(batch).squeeze(), batch.batch)\n",
    "                loss = loss_functin(output, y)\n",
    "                \n",
    "            batch_loss += loss.item()\n",
    "\n",
    "    print(\"EVAL LOSS\", batch_loss / len(test_ids))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083b341",
   "metadata": {},
   "source": [
    "Testing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d966391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphormer.layers import RadialBasisEmbedding, GraphormerEncoderLayer\n",
    "from graphormer.model import Graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7985462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# Node features (embeddings) for both graphs combined\n",
    "node_features = torch.rand((12, 64))  # 5 nodes from graph 1 + 7 nodes from graph 2, each with 64-dim features\n",
    "\n",
    "# Edge attributes for both graphs combined\n",
    "edge_attributes = torch.rand((12, 12, 16))  # 4 edges from graph 1 + 6 edges from graph 2, each with 16-dim attributes\n",
    "\n",
    "# Radial basis embedding for both graphs\n",
    "rb_embedding = torch.rand((12, 12, 5))  # 5 radial basis functions\n",
    "\n",
    "# Batch pointers\n",
    "ptr = torch.tensor([0, 5, 12])  # Pointers showing where each graph starts in the node features\n",
    "\n",
    "pos = torch.rand((12, 3))  # 5 nodes from graph 1 + 7 nodes from graph 2, each with 2-dim positions\n",
    "\n",
    "# Initialize GraphormerEncoderLayer\n",
    "# encoder_layer = GraphormerEncoderLayer(emb_dim=64, num_heads=8, num_radial=5, edge_attr_dim=16)\n",
    "# output = encoder_layer(node_features, rb_embedding, edge_attributes, ptr)\n",
    "# output.shape\n",
    "\n",
    "graphormer = Graphormer(\n",
    "    num_layers=3,\n",
    "    input_dim=64,\n",
    "    emb_dim=128,\n",
    "    input_edge_attr_dim=16,\n",
    "    edge_attr_dim=64,\n",
    "    output_dim=128,\n",
    "    num_radial=5,\n",
    "    radial_min=0,\n",
    "    radial_max=10,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "\n",
    "data = Data(\n",
    "    x = node_features,\n",
    "    edge_attr = edge_attributes,\n",
    "    ptr = ptr,\n",
    "    batch=torch.tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]),\n",
    "    pos = pos,\n",
    ")\n",
    "\n",
    "graphormer(data).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
